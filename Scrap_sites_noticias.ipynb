{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Cópia de Radar.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leandrominer85/Scrap_sites_noticias/blob/main/Scrap_sites_noticias.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAw_BYm9bntk"
      },
      "source": [
        "# **Projeto Scrap de sites de notícias**\n",
        "\n",
        "## Esse projeto tem como objetivo fazer a raspagem de dados de alguns sites de notícias. Esses scripts permitem fazer a raspagem dos dados das principais notícias tanto da home page quanto das 'últimas notícias'. Quando disponível utilizaremos o feed RSS.\n",
        "\n",
        "## Os sites que serão utilizados são : g1, r7, folha, estadão, metro, globo, uol, terra, msn, buzzfeed, veja, ig, antagonista, oglobo, ny_times, washington, cnn_br, cnn_en, bbc_br e bbc_en.\n",
        "\n",
        "## As saídas do projeto são  arquivos .xlsx para cada um dos sites com as seguintes colunas : \"fonte\", 'Título', 'Subtítulo', 'Link' e 'Hora'."
      ],
      "id": "sAw_BYm9bntk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5ND4NL1ebNs"
      },
      "source": [
        "Para rodar o script no colab são necessárias algumas instalações de módulos (selenium e chromer driver):"
      ],
      "id": "Z5ND4NL1ebNs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tY90c-v6JjB"
      },
      "source": [
        "!pip install selenium\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "from selenium import webdriver"
      ],
      "id": "4tY90c-v6JjB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loMit9rkfIiS"
      },
      "source": [
        "Agora podemos instanciar o chromedriver que será utilizado por todos os scripts que utilizam selenium:"
      ],
      "id": "loMit9rkfIiS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1a02637",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b112ff74-b07e-491b-91ed-6db52ba20947"
      },
      "source": [
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)"
      ],
      "id": "a1a02637",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: use options instead of chrome_options\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_P9wylafVuY"
      },
      "source": [
        "Importação dos módulos:"
      ],
      "id": "K_P9wylafVuY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "858b0c62"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import re\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from urllib.request import urlopen, Request\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from multiprocessing import Process\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "id": "858b0c62",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX_SUVy4ghTm"
      },
      "source": [
        "Temos 3 tipos de extração de dados: extração direta pelo beautifulsoup, extração utilizando o selenium + o beautifulsoup e extração por RSS.\n",
        "Mostrarei um exemplo com comentarios para cada uma das extrações nas próximas 2 funções. As demais seguem uma ou mais das extrações apenas com modificações relativas ao site e formatação."
      ],
      "id": "SX_SUVy4ghTm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03efcd64"
      },
      "source": [
        "def g1():\n",
        "    #Primeira parte: utilizando o BeautifulSoup para extração da homepage\n",
        "    \n",
        "    #criando uma lista vazia para receber as notícias\n",
        "    lista_noticias = []\n",
        "    \n",
        "    #Obtendo a resposta pela biblioteca requests\n",
        "    response = requests.get('https://g1.globo.com/')\n",
        "    \n",
        "    #Obtendo o conteúdo da resposta\n",
        "    content = response.content\n",
        "    \n",
        "    #Utilizando o BeautifulSoup para adquirir o conteúdo\n",
        "    site = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "    # HTML das notícias (procurando por todas as notícias com os dados que queremos)\n",
        "    noticias = site.findAll('div', attrs={'class': 'feed-post-body'})\n",
        "\n",
        "    #Iterando sobre as notícias\n",
        "    for noticia in noticias:\n",
        "      # Título\n",
        "        titulo = noticia.find('a', attrs={'class': 'feed-post-link'})\n",
        "\n",
        "        # Subtítulo: div class=\"feed-post-body-resumo\"\n",
        "        subtitulo = noticia.find('div', attrs={'class': 'feed-post-body-resumo'})\n",
        "\n",
        "        #O subtítulo nem sempre está presente, por isso é necessário esse if-else\n",
        "        if (subtitulo):\n",
        "            \n",
        "            lista_noticias.append([\"g1_home\",titulo.text, subtitulo.text, titulo['href'],\n",
        "                                   datetime.now().strftime((\"%d/%m/%Y %H:%M:%S\"))])\n",
        "        else:\n",
        "            lista_noticias.append([\"g1_home\",titulo.text, '', titulo['href'],\n",
        "                                   datetime.now().strftime((\"%d/%m/%Y %H:%M:%S\"))])\n",
        "\n",
        "\n",
        "    # Transformando a lista em dataframe\n",
        "    news1 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo', 'Link', \"Hora\"])\n",
        "    \n",
        "    \n",
        "    # Segunda parte da extração - utilizando o RSS\n",
        "    \n",
        "    #Reinstanciando a lista de noticias\n",
        "    lista_noticias = []\n",
        "    url = \"http://g1.globo.com/dynamo/rss2.xml\"\n",
        "    \n",
        "    #Obtendo a resposta pela biblioteca requests\n",
        "    response = requests.get(url)\n",
        "    \n",
        "    #Obtendo o conteúdo da resposta\n",
        "    site = BeautifulSoup(response.content, features='xml')\n",
        "    \n",
        "    # HTML das notícias (procurando por todas as notícias com os dados que queremos)\n",
        "    noticias = site.findAll('item')\n",
        "\n",
        "    #Iterando sobre as notícias\n",
        "    for noticia in noticias:\n",
        "      # Título\n",
        "        titulo = noticia.find('title')\n",
        "        \n",
        "        #O subtítulo nem sempre está presente, por isso é necessário esse if-else\n",
        "        subtitulo = re.sub(r'<.+?>', '', noticia.find('description').text).strip()\n",
        "\n",
        "        if (subtitulo):\n",
        "\n",
        "            #Adiciono uma coluna com o nome da fonte, o título, o subtítulo e a hora do acesso com o módulo time\n",
        "            lista_noticias.append([\"g1_ultimas\", titulo.text, subtitulo,\n",
        "                                   noticia.find(\"link\").text, datetime.now().strftime((\"%d/%m/%Y %H:%M:%S\"))])\n",
        "        else:\n",
        "            lista_noticias.append([\"g1_ultimas\" ,titulo.text, '',\n",
        "                                   noticia.find(\"link\").text, datetime.now().strftime((\"%d/%m/%Y %H:%M:%S\"))])\n",
        "            \n",
        "    # Transformando a lista em dataframe\n",
        "    news2 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo', 'Link', \"Hora\"])\n",
        "    \n",
        "    #Concatenando os dois dataframes\n",
        "    df = pd.concat([news1,news2])\n",
        "    \n",
        "    #Salvando o dataframe\n",
        "    df.to_excel('g1.xlsx', index=False)"
      ],
      "id": "03efcd64",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3fa1ab2"
      },
      "source": [
        "def buzz():\n",
        "    #o site utiliza javascript, por isso é preciso utilizar a biblioteca selenium\n",
        "    # Refs: https://medium.com/data-hackers/web-scraping-com-python-para-pregui%C3%A7osos-unindo-beautifulsoup-e-selenium-parte-2-8cfebf4f34e\n",
        "    # https://blog.4linux.com.br/web-scraping-python-selenium-e-beautifulsoup/\n",
        "\n",
        "    #Iniciando a lista vazia\n",
        "    lista_noticias= []\n",
        "    \n",
        "    url = (\"https://www.buzzfeednews.com/\")\n",
        "\n",
        "    #acessando a url (utilizando o driver definido acima)\n",
        "    driver.get(url)\n",
        "\n",
        "    #Esperando o carregamento\n",
        "    time.sleep(5)\n",
        "\n",
        "    #Procurando o elemento da página\n",
        "    element = driver.find_element_by_class_name('splash-regular-stories')\n",
        "\n",
        "    #Carregando o html\n",
        "    html = element.get_attribute(\"innerHTML\")\n",
        "\n",
        "    #Utilizando o BeautifulSoup para acessar os dados\n",
        "    site = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Neste site há quatro tipos de noticias mas todas seguem o esquema da principal\n",
        "    \n",
        "    #Obtendo as noticias\n",
        "    principal = site.find('a',\n",
        "                          attrs={'class': \"splash-regular-stories__primary-story-image-link xs-block splash-regular-stories__link\"})\n",
        "\n",
        "    #Buscando o título\n",
        "    titulo = site.find('h2',attrs={'class': \"splash-regular-stories__primary-story-title\"}).text.strip()\n",
        "\n",
        "    #Apendando à lista\n",
        "    lista_noticias.append([\"buzz_home\",titulo, '', principal['href'],\n",
        "                           datetime.now().strftime((\"%d/%m/%Y %H:%M:%S\"))])\n",
        "\n",
        "\n",
        "    secundaria = site.find('a',\n",
        "                           attrs={'class': \"splash-regular-stories__link xs-flex-align-start xs-flex-justify-space-between\"})\n",
        "\n",
        "    titulo= site.find('h2',attrs={'class': \"splash-regular-stories__secondary-story-title\"}).text.strip()\n",
        "\n",
        "    lista_noticias.append([\"buzz_home\",titulo, '', secundaria['href'],\n",
        "                           datetime.now().strftime((\"%d/%m/%Y %H:%M:%S\"))])\n",
        "\n",
        "\n",
        "    terciaria = site.find('a',\n",
        "                          attrs={'class': \"xs-flex splash-regular-stories__link xs-flex-align-start xs-flex-justify-space-between\"})\n",
        "\n",
        "    titulo= site.find('h2',attrs={'class': \"splash-regular-stories__tertiary-story-title xs-pr1 md-pr0\"}).text.strip()\n",
        "\n",
        "    lista_noticias.append([\"buzz_home\",titulo, '', terciaria['href'],\n",
        "                           datetime.now().strftime((\"%d/%m/%Y %H:%M:%S\"))])\n",
        "\n",
        "\n",
        "    #Procurando o elemento da página\n",
        "    element = driver.find_element_by_class_name('sm-mt2')\n",
        "\n",
        "    #Carregando o html\n",
        "    html = element.get_attribute(\"innerHTML\")\n",
        "\n",
        "    #Utilizando o BeautifulSoup para acessar os dados\n",
        "    site = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "\n",
        "    # HTML da notícia\n",
        "    noticias = site.findAll('a', attrs={'class': \"newsblock-story-card__link xs-flex\"})\n",
        "\n",
        "    #Iterando sobre as notícias\n",
        "    for noticia in noticias:\n",
        "      # Título\n",
        "        titulo = noticia.text\n",
        "\n",
        "\n",
        "        #Populando a lista com os resultados\n",
        "        lista_noticias.append([\"buzz_home\",titulo, '', noticia['href'],\n",
        "                               datetime.now().strftime((\"%d/%m/%Y %H:%M:%S\"))])\n",
        "\n",
        "    #Transformando a lista em Dataframe e salvando como .xlsx \n",
        "    news1 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo', 'Link', \"Hora\"])\n",
        "    \n",
        "    news1.to_excel('buzz.xlsx', index=False)"
      ],
      "id": "f3fa1ab2",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e19f5b8"
      },
      "source": [
        "def r7():\n",
        "    \n",
        "\n",
        "    lista_noticias = []\n",
        "\n",
        "    response = requests.get('https://www.r7.com/')\n",
        "\n",
        "    content = response.content\n",
        "\n",
        "    site = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "    # HTML da notícia\n",
        "    noticias = site.findAll('div', attrs={'class': 'r7-flex-hat'})\n",
        "\n",
        "    for noticia in noticias:\n",
        "        try:\n",
        "          # Título\n",
        "            titulo = noticia.find('a', attrs={'class': 'r7-flex-hat__description'})\n",
        "\n",
        "            # Subtítulo: div class=\"feed-post-body-resumo\"\n",
        "            subtitulo = titulo['title']\n",
        "\n",
        "            if (subtitulo):\n",
        "                # print(subtitulo.text)\n",
        "                lista_noticias.append(['r7_home',titulo.text, subtitulo, titulo['href']])\n",
        "            else:\n",
        "                lista_noticias.append(['r7_home',titulo.text, '', titulo['href']])\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "\n",
        "    news1 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo', 'Link'])\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    lista_noticias = []\n",
        "    url = \"https://noticias.r7.com/feed.xml\"\n",
        "    response = requests.get(url)\n",
        "    site = BeautifulSoup(response.content, features='xml')\n",
        "    noticias = site.findAll('entry')\n",
        "\n",
        "    for noticia in noticias:\n",
        "      # Título\n",
        "        titulo = noticia.find('title').text\n",
        "\n",
        "        # Subtítulo: div class=\"feed-post-body-resumo\"\n",
        "        subtitulo = re.sub(r'<.+?>', '', noticia.find('content').text).strip()\n",
        "\n",
        "        if (subtitulo):\n",
        "\n",
        "            lista_noticias.append([\"r7_ultimas\", titulo, subtitulo, noticia.find(\"link\")['href']])\n",
        "        else:\n",
        "            lista_noticias.append([\"r7_ultimas\" ,titulo, '', noticia.find(\"link\")['href']])\n",
        "    \n",
        "    news2 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo', 'Link'])\n",
        "    \n",
        "    df = pd.concat([news1,news2])\n",
        "    \n",
        "    df.to_excel('r7.xlsx', index=False)"
      ],
      "id": "9e19f5b8",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f67b909f"
      },
      "source": [
        "def folha():\n",
        "    lista_noticias = []\n",
        "\n",
        "    response = requests.get('https://www.folha.uol.com.br/')\n",
        "\n",
        "    content = response.content\n",
        "\n",
        "    site = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "    # HTML da notícia\n",
        "    noticias = site.findAll('div', attrs={'class': 'row'})\n",
        "\n",
        "    for noticia in noticias:\n",
        "      # Título\n",
        "        try:\n",
        "            titulo = noticia.find('h2', attrs={'class': \"c-main-headline__title\"}).text.strip()\n",
        "        except:\n",
        "            pass\n",
        "        try:\n",
        "            titulo = noticia.find('h2', attrs={'class': \"c-headline__title\"}).text.strip()\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "\n",
        "        subtitulo = noticia.find('p', attrs={'class': 'c-headline__standfirst'})\n",
        "\n",
        "        if (subtitulo):\n",
        "\n",
        "            lista_noticias.append([\"folha_home\",titulo, subtitulo.text, noticia.find('a')['href']])\n",
        "        else:\n",
        "            lista_noticias.append([\"folha_home\",titulo, '', noticia.find('a')['href']])\n",
        "\n",
        "\n",
        "    news1 = pd.DataFrame(lista_noticias,\n",
        "                         columns=[\"fonte\",'Título', 'Subtítulo', 'Link']).drop_duplicates(subset= \"Título\",keep='last')\n",
        "\n",
        "    lista_noticias = []\n",
        "    url = \"http://feeds.folha.uol.com.br/emcimadahora/rss091.xml\"\n",
        "    response = requests.get(url)\n",
        "    site = BeautifulSoup(response.content, features='xml')\n",
        "    noticias = site.findAll('item')\n",
        "\n",
        "    for noticia in noticias:\n",
        "      # Título\n",
        "        titulo = noticia.find('title')\n",
        "\n",
        "        # Subtítulo: div class=\"feed-post-body-resumo\"\n",
        "        subtitulo = re.sub(r'<.+?>', '', noticia.find('description').text).strip()\n",
        "\n",
        "        if (subtitulo):\n",
        "\n",
        "            lista_noticias.append([\"folha_ultimas\", titulo.text, subtitulo, noticia.find(\"link\").text])\n",
        "        else:\n",
        "            lista_noticias.append([\"folha_ultimas\" ,titulo.text, '', noticia.find(\"link\").text])\n",
        "\n",
        "    news2 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo', 'Link'])\n",
        "\n",
        "    df = pd.concat([news1,news2])\n",
        "\n",
        "    df.to_excel('folha.xlsx', index=False)"
      ],
      "id": "f67b909f",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94c7775a"
      },
      "source": [
        "def estadao():\n",
        "    lista_noticias = []\n",
        "\n",
        "    response = requests.get('https://www.estadao.com.br/')\n",
        "\n",
        "    content = response.content\n",
        "\n",
        "    site = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "    # HTML da notícia\n",
        "    noticias = site.findAll('div', attrs={'class': 'intro'})\n",
        "\n",
        "    for noticia in noticias:\n",
        "      # Título\n",
        "        try:\n",
        "            titulo = noticia.find('h3', attrs={'class': \"title\"}).text.strip()\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "\n",
        "        subtitulo = noticia.find('p', attrs={'class': 'c-headline__standfirst'})\n",
        "\n",
        "        if (subtitulo):\n",
        "\n",
        "            lista_noticias.append([\"estadao_home\",titulo, subtitulo.text, noticia.find('a')['href']])\n",
        "        else:\n",
        "            lista_noticias.append([\"estadao_home\",titulo, '', noticia.find('a')['href']])\n",
        "\n",
        "\n",
        "    news1 = pd.DataFrame(lista_noticias,columns=[\"fonte\",'Título', 'Subtítulo', 'Link'])\n",
        "\n",
        "\n",
        "    lista_noticias = []\n",
        "\n",
        "    response = requests.get('https://www.estadao.com.br/ultimas')\n",
        "\n",
        "    content = response.content\n",
        "\n",
        "    site = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "    # HTML da notícia\n",
        "    noticias = site.findAll('div', attrs={'class': 'row'})\n",
        "\n",
        "    for noticia in noticias:\n",
        "      # Título\n",
        "        try:\n",
        "            titulo = noticia.find('h3', attrs={'class': \"third\"}).text.strip()\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "\n",
        "        subtitulo = noticia.find('p', attrs={'class': 'c-headline__standfirst'})\n",
        "\n",
        "        if (subtitulo):\n",
        "\n",
        "            lista_noticias.append([\"estadao_ultimas\",titulo, subtitulo.text, noticia.find('a')['href']])\n",
        "        else:\n",
        "            lista_noticias.append([\"estadao_ultimas\",titulo, '', noticia.find('a')[\"href\"]])\n",
        "\n",
        "\n",
        "    news2 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo',\n",
        "                                                  'Link']).drop_duplicates(subset= \"Título\",keep='last')\n",
        "\n",
        "    df = pd.concat([news1,news2])\n",
        "\n",
        "    df.to_excel('estadao.xlsx', index=False)"
      ],
      "id": "94c7775a",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "468d3fa5"
      },
      "source": [
        "def metro():\n",
        "    lista_noticias = []\n",
        "    \n",
        "    response = requests.get('https://www.metropoles.com/')\n",
        "\n",
        "    content = response.content\n",
        "\n",
        "    site = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "    # HTML da notícia\n",
        "    noticias = site.findAll('div', attrs={'class': 'm-box-text'})\n",
        "\n",
        "    for noticia in noticias:\n",
        "      # Título\n",
        "        try:\n",
        "            titulo = noticia.find_all('a')[1].text\n",
        "        except:\n",
        "                continue\n",
        "\n",
        "        try:\n",
        "            subtitulo = noticia.find(\"p\").text\n",
        "        except:\n",
        "            subtitulo = None\n",
        "\n",
        "        if (subtitulo):\n",
        "\n",
        "            lista_noticias.append([\"metro_home\",titulo, subtitulo, noticia.find_all('a')[1]['href']])\n",
        "        else:\n",
        "            lista_noticias.append([\"metro_home\",titulo, '', noticia.find_all('a')[1]['href']])\n",
        "\n",
        "\n",
        "    news1 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo', 'Link'])\n",
        "\n",
        "\n",
        "\n",
        "    lista_noticias = []\n",
        "\n",
        "    response = requests.get('https://www.metropoles.com/ultimas-noticias')\n",
        "\n",
        "    content = response.content\n",
        "\n",
        "    site = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "    # HTML da notícia\n",
        "    noticias = site.findAll('div', attrs={'class': 'column is-full is-full'})\n",
        "\n",
        "    for noticia in noticias:\n",
        "      # Título\n",
        "        try:\n",
        "            titulo = noticia.find_all('a')[2].text\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "\n",
        "        subtitulo = noticia.find('p', attrs={'class': 'm-resume'}).text\n",
        "\n",
        "        if (subtitulo):\n",
        "\n",
        "            lista_noticias.append([\"metro_ultimas\",titulo, subtitulo, noticia.find('a')['href']])\n",
        "        else:\n",
        "            lista_noticias.append([\"metro_ultimas\",titulo, '', noticia.find('a')[\"href\"]])\n",
        "\n",
        "\n",
        "    news2 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo',\n",
        "                                                  'Link']).drop_duplicates(subset= \"Título\",keep='last')\n",
        "\n",
        "    df = pd.concat([news1,news2])\n",
        "\n",
        "    df.to_excel('metropoles.xlsx', index=False)"
      ],
      "id": "468d3fa5",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3c4159d"
      },
      "source": [
        "def globo():\n",
        "    #o site utiliza javascript, por isso é preciso utilizar a biblioteca selenium\n",
        "    # Refs: https://medium.com/data-hackers/web-scraping-com-python-para-pregui%C3%A7osos-unindo-beautifulsoup-e-selenium-parte-2-8cfebf4f34e\n",
        "    # https://blog.4linux.com.br/web-scraping-python-selenium-e-beautifulsoup/\n",
        "    \n",
        "    \n",
        "\n",
        "    url = (\"https://www.globo.com/\")\n",
        "    \n",
        "    #acessando a url\n",
        "    driver.get(url)\n",
        "    \n",
        "    #Esperando o carregamento\n",
        "    time.sleep(10)\n",
        "\n",
        "    #Procurando o elemento da página\n",
        "    element = driver.find_element_by_class_name('area-destaque')\n",
        "    \n",
        "    #Carregando o html\n",
        "    html = element.get_attribute(\"innerHTML\")\n",
        "\n",
        "    #Utilizando o BeautifulSoup para acessar os dados\n",
        "    site = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "    lista_noticias=[]\n",
        "\n",
        "    # HTML da notícia\n",
        "    noticias = site.findAll('a', attrs={'class': \"post__link\"})\n",
        "\n",
        "    for noticia in noticias:\n",
        "      # Título\n",
        "        titulo = noticia['title']\n",
        "\n",
        "        # Subtítulo: div class=\"feed-post-body-resumo\"\n",
        "        subtitulo = noticia.text\n",
        "\n",
        "        if (subtitulo):\n",
        "            # print(subtitulo.text)\n",
        "            lista_noticias.append([\"globo_home\",titulo, subtitulo, noticia['data-tracking-label']])\n",
        "        else:\n",
        "            lista_noticias.append([\"globo_home\",titulo, '', noticia['data-tracking-label']])\n",
        "\n",
        "\n",
        "    news1 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo', 'Link'])\n",
        "\n",
        "    lista_noticias = []\n",
        "    url = \"https://oglobo.globo.com/rss.xml?completo=true\"\n",
        "    response = requests.get(url)\n",
        "    site = BeautifulSoup(response.content, features='xml')\n",
        "    noticias = site.findAll('item')\n",
        "\n",
        "    for noticia in noticias:\n",
        "      # Título\n",
        "        titulo = noticia.find('title')\n",
        "\n",
        "        # Subtítulo: div class=\"feed-post-body-resumo\"\n",
        "        subtitulo = re.sub(r'<.+?>', '', noticia.find('description').text).strip()\n",
        "\n",
        "        if (subtitulo):\n",
        "\n",
        "            lista_noticias.append([\"globo_ultimas\", titulo.text, subtitulo, noticia.find(\"link\").text])\n",
        "        else:\n",
        "            lista_noticias.append([\"globo_ultimas\" ,titulo.text, '', noticia.find(\"link\").text])\n",
        "\n",
        "    news2 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo', 'Link'])\n",
        "\n",
        "    df = pd.concat([news1,news2])\n",
        "\n",
        "    df.to_excel('globo.xlsx', index=False)"
      ],
      "id": "b3c4159d",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0503e7a4"
      },
      "source": [
        "def uol():\n",
        "    lista_noticias = []\n",
        "\n",
        "    response = requests.get('https://noticias.uol.com.br/')\n",
        "\n",
        "    content = response.content\n",
        "\n",
        "    site = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "\n",
        "    # HTML da notícia\n",
        "    high = site.findAll('div', attrs={'class': 'highlights'})\n",
        "\n",
        "    for h in high:\n",
        "      # Título\n",
        "        titulo = h.text\n",
        "\n",
        "        lista_noticias.append([\"uol_home\",titulo, '', h.find(\"a\")['href']])\n",
        "\n",
        "\n",
        "\n",
        "    # HTML da notícia\n",
        "    noticias = site.findAll('div', attrs={'class': 'thumbnails-wrapper'})\n",
        "\n",
        "    for noticia in noticias:\n",
        "      # Título\n",
        "        titulo = noticia.text\n",
        "\n",
        "        lista_noticias.append([\"uol_home\",titulo, '', noticia.find(\"a\")['href']])\n",
        "\n",
        "\n",
        "    news1 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo', 'Link'])\n",
        "\n",
        "    lista_noticias = []\n",
        "    url = \"http://rss.uol.com.br/feed/noticias.xml\"\n",
        "    response = requests.get(url)\n",
        "    site = BeautifulSoup(response.content, features='xml')\n",
        "    noticias = site.findAll('item')\n",
        "\n",
        "    for noticia in noticias:\n",
        "      # Título\n",
        "        titulo = noticia.find('title')\n",
        "\n",
        "        # Subtítulo: div class=\"feed-post-body-resumo\"\n",
        "        subtitulo = re.sub(r'<.+?>', '', noticia.find('description').text).strip()\n",
        "\n",
        "        if (subtitulo):\n",
        "\n",
        "            lista_noticias.append([\"uol_ultimas\", titulo.text, subtitulo, noticia.find(\"link\").text])\n",
        "        else:\n",
        "            lista_noticias.append([\"uol_ultimas\" ,titulo.text, '', noticia.find(\"link\").text])\n",
        "\n",
        "    news2 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo', 'Link'])\n",
        "\n",
        "    df = pd.concat([news1,news2])\n",
        "\n",
        "    df.to_excel('uol.xlsx', index=False)"
      ],
      "id": "0503e7a4",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a3630fc"
      },
      "source": [
        "def terra():\n",
        "    lista_noticias = []\n",
        "\n",
        "    response = requests.get('https://www.terra.com.br/')\n",
        "\n",
        "    content = response.content\n",
        "\n",
        "    site = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "    # HTML da notícia\n",
        "    noticias = site.findAll('a', attrs={'class': 'card-news__text--title main-url'})\n",
        "\n",
        "    for noticia in noticias:\n",
        "      # Título\n",
        "        titulo = noticia.text\n",
        "\n",
        "        lista_noticias.append([\"terra_home\",titulo, '', noticia['href']])\n",
        "\n",
        "\n",
        "    # HTML da notícia\n",
        "    noticias = site.findAll('a', attrs={'class': 'card-h-small__text--title main-url'})\n",
        "\n",
        "    for noticia in noticias:\n",
        "      # Título\n",
        "        titulo = noticia.text\n",
        "\n",
        "        lista_noticias.append([\"terra_home\",titulo, '', noticia['href']])\n",
        "\n",
        "\n",
        "\n",
        "    news1 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo', 'Link'])\n",
        "\n",
        "\n",
        "\n",
        "    lista_noticias = []\n",
        "\n",
        "    response = requests.get('https://www.terra.com.br/noticias/')\n",
        "    content = response.content\n",
        "\n",
        "    site = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "    # HTML da notícia\n",
        "    noticias = site.findAll('a', attrs={'class': 'card-news__text--title main-url'})\n",
        "\n",
        "    for noticia in noticias:\n",
        "      # Título\n",
        "        titulo = noticia.text\n",
        "\n",
        "        lista_noticias.append([\"terra_noticias\",titulo, '', noticia['href']])\n",
        "\n",
        "\n",
        "    # HTML da notícia\n",
        "    noticias = site.findAll('a', attrs={'class': 'card-h-small__text--title main-url'})\n",
        "\n",
        "    for noticia in noticias:\n",
        "      # Título\n",
        "        titulo = noticia.text\n",
        "\n",
        "        lista_noticias.append([\"terra_noticias\",titulo, '', noticia['href']])\n",
        "\n",
        "\n",
        "\n",
        "    news2 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo', 'Link'])\n",
        "\n",
        "\n",
        "    df = pd.concat([news1,news2])\n",
        "\n",
        "    df.to_excel('terra.xlsx', index=False)"
      ],
      "id": "8a3630fc",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f81085f8"
      },
      "source": [
        "def msn():\n",
        "    \n",
        "\n",
        "    lista_noticias = []\n",
        "    url = \"https://rss.msn.com/pt-br/\"\n",
        "    response = requests.get(url)\n",
        "    site = BeautifulSoup(response.content, features='xml')\n",
        "    noticias = site.findAll('item')\n",
        "\n",
        "    for noticia in noticias:\n",
        "      # Título\n",
        "        titulo = noticia.find('title')\n",
        "\n",
        "\n",
        "        lista_noticias.append([\"msn_ultimas\" ,titulo.text, '', noticia.find(\"link\").text])\n",
        "\n",
        "    news2 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo', 'Link'])\n",
        "\n",
        "    news2.to_excel('msn.xlsx', index=False)"
      ],
      "id": "f81085f8",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "86c199b2"
      },
      "source": [
        "def veja():\n",
        "    #o site utiliza javascript, por isso é preciso utilizar a biblioteca selenium\n",
        "    # Refs: https://medium.com/data-hackers/web-scraping-com-python-para-pregui%C3%A7osos-unindo-beautifulsoup-e-selenium-parte-2-8cfebf4f34e\n",
        "    # https://blog.4linux.com.br/web-scraping-python-selenium-e-beautifulsoup/\n",
        "\n",
        "      \n",
        "\n",
        "  url = (\"https://veja.abril.com.br/\")\n",
        "\n",
        "  #acessando a url\n",
        "  driver.get(url)\n",
        "\n",
        "  #Esperando o carregamento\n",
        "  time.sleep(3)\n",
        "\n",
        "  #Procurando o elemento da página\n",
        "  if driver.find_element_by_xpath(\"/html/body/main/section[1]\").get_attribute(\"innerHTML\").find('block edicao-semana dark') != None:\n",
        "    element = driver.find_element_by_xpath(\"//section[2]\")\n",
        "  else:\n",
        "    element = driver.find_element_by_xpath(\"//section[1]\")\n",
        "\n",
        "\n",
        "\n",
        "  #Carregando o html\n",
        "  html = element.get_attribute(\"innerHTML\")\n",
        "\n",
        "  #Utilizando o BeautifulSoup para acessar os dados\n",
        "  site = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "  lista_noticias=[]\n",
        "\n",
        "  if site.find('div', attrs={'class': \"card d\"}) == None:\n",
        "    element = driver.find_element_by_xpath(\"//section[1]\")\n",
        "    #Carregando o html\n",
        "    html = element.get_attribute(\"innerHTML\")\n",
        "\n",
        "    #Utilizando o BeautifulSoup para acessar os dados\n",
        "    site = BeautifulSoup(html, \"html.parser\")\n",
        "    \n",
        "  principal = site.find('div', attrs={'class': \"card d\"})\n",
        "  titulo = principal.find('h2').text\n",
        "  subtitulo = principal.find('p').text\n",
        "  lista_noticias.append([\"veja_home\",titulo, subtitulo, principal.find('a')['href']])\n",
        "\n",
        "\n",
        "  secundarias = site.findAll('div', attrs={'class': \"col-s-12 col-l-4\"})\n",
        "\n",
        "  for noticia in secundarias:\n",
        "    titulo= noticia.find(\"h3\").text\n",
        "\n",
        "\n",
        "    lista_noticias.append([\"veja_home\",titulo, '', noticia.find('a')['href']])\n",
        "\n",
        "\n",
        "\n",
        "  terciarias = site.findAll('a', attrs={'class': \"link related-article\"})\n",
        "\n",
        "  for noticia in terciarias:\n",
        "\n",
        "\n",
        "    titulo= noticia.text\n",
        "\n",
        "    lista_noticias.append([\"veja_home\",titulo, subtitulo, noticia['href']])\n",
        "\n",
        "\n",
        "  news1 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo', 'Link'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  url = (\"https://veja.abril.com.br/ultimas-noticias/\")\n",
        "\n",
        "  #acessando a url\n",
        "  driver.get(url)\n",
        "\n",
        "  #Esperando o carregamento\n",
        "  time.sleep(3)\n",
        "\n",
        "  #Procurando o elemento da página\n",
        "  element = driver.find_element_by_class_name('list')\n",
        "\n",
        "  #Carregando o html\n",
        "  html = element.get_attribute(\"innerHTML\")\n",
        "\n",
        "  #Utilizando o BeautifulSoup para acessar os dados\n",
        "  site = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "\n",
        "  # HTML da notícia\n",
        "  noticias =site.findAll('div', attrs={'class': \"row\"})\n",
        "\n",
        "  for noticia in noticias:\n",
        "    # Título\n",
        "      titulo = noticia.find('h2',  attrs={'class': \"title\"}).text\n",
        "\n",
        "\n",
        "      lista_noticias.append([\"veja_ultimas\",titulo, '', noticia.find('a')['href']])\n",
        "\n",
        "\n",
        "  news2 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo', 'Link']).drop_duplicates(subset= \"Título\",keep='last')\n",
        "\n",
        "\n",
        "  news2.to_excel('veja.xlsx', index=False)\n"
      ],
      "id": "86c199b2",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "415253a0"
      },
      "source": [
        "def ig():\n",
        "\n",
        "    lista_noticias = []\n",
        "\n",
        "    response = requests.get('https://www.ig.com.br/')\n",
        "\n",
        "    content = response.content\n",
        "\n",
        "    site = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "    # principais\n",
        "    main = site.find('div', attrs={'class': 'destaque_containerText'})\n",
        "\n",
        "    principais = main.find_all('a', attrs={'data-tb-region-item': 'taboola-region'})\n",
        "    for noticia in principais:\n",
        "    # Título\n",
        "        titulo = noticia.text\n",
        "\n",
        "\n",
        "        lista_noticias.append([\"ig_home\",titulo, '', noticia['href']])\n",
        "\n",
        "    # HTML da notícia\n",
        "    noticias = site.findAll('a', attrs={'class': 'title'})\n",
        "\n",
        "    for noticia in noticias:\n",
        "        \n",
        "    # Título\n",
        "        titulo = noticia.text\n",
        "\n",
        "\n",
        "        lista_noticias.append([\"ig_home\",titulo, '', noticia['href']])\n",
        "\n",
        "\n",
        "    news1 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo', 'Link'])\n",
        "\n",
        "\n",
        "\n",
        "    #o site utiliza javascript, por isso é preciso utilizar a biblioteca selenium\n",
        "    # Refs: https://medium.com/data-hackers/web-scraping-com-python-para-pregui%C3%A7osos-unindo-beautifulsoup-e-selenium-parte-2-8cfebf4f34e\n",
        "    # https://blog.4linux.com.br/web-scraping-python-selenium-e-beautifulsoup/\n",
        "\n",
        "    lista_noticias=[]\n",
        "\n",
        "\n",
        "\n",
        "    url = (\"https://ultimosegundo.ig.com.br/\")\n",
        "\n",
        "    #acessando a url\n",
        "    driver.get(url)\n",
        "\n",
        "    #Esperando o carregamento\n",
        "    time.sleep(5)\n",
        "\n",
        "    #Procurando o elemento da página\n",
        "    element = driver.find_element_by_class_name('ultimosegundo')\n",
        "\n",
        "    #Carregando o html\n",
        "    html = element.get_attribute(\"innerHTML\")\n",
        "\n",
        "    #Utilizando o BeautifulSoup para acessar os dados\n",
        "    site = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "\n",
        "    noticias = site.findAll('a', attrs={'class': 'destaque-item-link'})\n",
        "\n",
        "    for noticia in noticias:\n",
        "    # Título\n",
        "        titulo = noticia.find(\"img\")['title']\n",
        "\n",
        "\n",
        "        lista_noticias.append([\"ig_ultimas\" ,titulo, '', noticia['href']])\n",
        "\n",
        "    news2 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo', 'Link'])\n",
        "\n",
        "    df = pd.concat([news1,news2])\n",
        "\n",
        "    df.to_excel('ig.xlsx', index=False)"
      ],
      "id": "415253a0",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2492af4f"
      },
      "source": [
        "def antagonista ():\n",
        "    \n",
        "    lista_noticias = []\n",
        "    url = (\"https://www.oantagonista.com/brasil/\")\n",
        "\n",
        "    #acessando a url\n",
        "    driver.get(url)\n",
        "\n",
        "    #Esperando o carregamento\n",
        "    time.sleep(5)\n",
        "\n",
        "\n",
        "    elem = driver.find_element_by_xpath(\"/html/body\")\n",
        "\n",
        "\n",
        "    #loading pages (https://stackoverflow.com/questions/21006940/how-to-load-all-entries-in-an-infinite-scroll-at-once-to-parse-the-html-in-pytho)\n",
        "    no_of_pagedowns = 5\n",
        "\n",
        "    while no_of_pagedowns:\n",
        "        elem.send_keys(Keys.PAGE_DOWN)\n",
        "        time.sleep(0.3)\n",
        "        no_of_pagedowns-=1\n",
        "\n",
        "\n",
        "    #Procurando o elemento da página\n",
        "    element = driver.find_element_by_class_name('container_home_left')\n",
        "\n",
        "    #Carregando o html\n",
        "    html = element.get_attribute(\"innerHTML\")\n",
        "\n",
        "    #Utilizando o BeautifulSoup para acessar os dados\n",
        "    site = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "\n",
        "    # HTML da notícia\n",
        "    noticias = site.findAll('div', attrs={'class': 'post_header'})\n",
        "\n",
        "    for noticia in noticias:\n",
        "        # Título\n",
        "        titulo = noticia.find('a')['title']\n",
        "        subtitulo = noticia.find('p').text\n",
        "\n",
        "        lista_noticias.append([\"antagonista_brasil\",titulo, subtitulo, noticia.find('a')['href']])\n",
        "\n",
        "\n",
        "    news1 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo', 'Link'])\n",
        "    news1.to_excel('antagonista.xlsx', index=False)"
      ],
      "id": "2492af4f",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e3e4f87"
      },
      "source": [
        "def oglobo():\n",
        "    lista_noticias = []\n",
        "\n",
        "    response = requests.get('https://oglobo.globo.com/ultimas-noticias/')\n",
        "\n",
        "    content = response.content\n",
        "\n",
        "    site = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "\n",
        "    principal = site.find('h1', attrs={'class': 'teaser__title headline__title'})\n",
        "    titulo = principal.text.strip()\n",
        "    lista_noticias.append([\"oglobo_ultimas\",titulo, '', principal.find('a')['href']])\n",
        "\n",
        "    # HTML da notícia\n",
        "    noticias = site.findAll('h1', attrs={'class': 'teaser__title teaser__title'})\n",
        "\n",
        "    for noticia in noticias:\n",
        "    # Título\n",
        "        titulo = noticia.text.strip()\n",
        "\n",
        "        lista_noticias.append([\"oglobo_ultimas\",titulo, '', noticia.find('a')['href']])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    news1 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo', 'Link'])\n",
        "\n",
        "\n",
        "    lista_noticias = []\n",
        "    url = \"https://oglobo.globo.com/rss.xml?completo=true\"\n",
        "    response = requests.get(url)\n",
        "    site = BeautifulSoup(response.content, features='xml')\n",
        "    noticias = site.findAll('item')\n",
        "\n",
        "    for noticia in noticias:\n",
        "    # Título\n",
        "        titulo = noticia.find('title')\n",
        "\n",
        "        # Subtítulo: div class=\"feed-post-body-resumo\"\n",
        "        subtitulo = re.sub(r'<.+?>', '', noticia.find('description').text).strip()\n",
        "\n",
        "        if (subtitulo):\n",
        "\n",
        "            lista_noticias.append([\"oglobo_rss\", titulo.text, subtitulo, noticia.find(\"link\").text])\n",
        "        else:\n",
        "            lista_noticias.append([\"oglobo_rss\" ,titulo.text, '', noticia.find(\"link\").text])\n",
        "\n",
        "    news2 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo', 'Link'])\n",
        "\n",
        "\n",
        "    df = pd.concat([news1 , news2])\n",
        "    df.to_excel('oglobo.xlsx', index=False)"
      ],
      "id": "1e3e4f87",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18a1580e"
      },
      "source": [
        "def ny_times():\n",
        "    lista_noticias = []\n",
        "    url = \"https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml\"\n",
        "    response = requests.get(url)\n",
        "    site = BeautifulSoup(response.content, features='xml')\n",
        "    noticias = site.findAll('item')\n",
        "\n",
        "    for noticia in noticias:\n",
        "      # Título\n",
        "        titulo = noticia.find('title')\n",
        "\n",
        "        # Subtítulo: div class=\"feed-post-body-resumo\"\n",
        "        subtitulo = re.sub(r'<.+?>', '', noticia.find('description').text).strip()\n",
        "\n",
        "        if (subtitulo):\n",
        "\n",
        "            lista_noticias.append([\"ny_times\", titulo.text, subtitulo, noticia.find(\"link\").text])\n",
        "        else:\n",
        "            lista_noticias.append([\"ny_times\" ,titulo.text, '', noticia.find(\"link\").text])\n",
        "\n",
        "    news2 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo', 'Link'])\n",
        "\n",
        "\n",
        "    news2.to_excel('ny_times.xlsx', index=False)"
      ],
      "id": "18a1580e",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98774662"
      },
      "source": [
        "def washington():\n",
        "    #o site utiliza javascript, por isso é preciso utilizar a biblioteca selenium\n",
        "    # Refs: https://medium.com/data-hackers/web-scraping-com-python-para-pregui%C3%A7osos-unindo-beautifulsoup-e-selenium-parte-2-8cfebf4f34e\n",
        "    # https://blog.4linux.com.br/web-scraping-python-selenium-e-beautifulsoup/\n",
        "\n",
        "    lista_noticias=[]\n",
        "\n",
        "\n",
        "\n",
        "    url = (\"https://www.washingtonpost.com/\")\n",
        "\n",
        "    #acessando a url\n",
        "    driver.get(url)\n",
        "\n",
        "    #Esperando o carregamento\n",
        "    time.sleep(5)\n",
        "\n",
        "    #Procurando o elemento da página\n",
        "    element = driver.find_element_by_xpath(\"/html/body/div[1]/div/main\")\n",
        "\n",
        "    #Carregando o html\n",
        "    html = element.get_attribute(\"innerHTML\")\n",
        "\n",
        "    #Utilizando o BeautifulSoup para acessar os dados\n",
        "    site = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "\n",
        "    noticias = site.findAll('div', attrs={'class': 'card-top card-text'}) \n",
        "\n",
        "    for noticia in noticias:\n",
        "    # Título\n",
        "        try:\n",
        "            titulo = noticia.find(\"span\").text\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "\n",
        "        lista_noticias.append([\"washington\" ,titulo, '',noticia.find(\"a\")['href']])\n",
        "\n",
        "    news2 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo', 'Link'])\n",
        "    news2 = news2[news2['Título'].str.len() > 20]\n",
        "\n",
        "    news2.to_excel('washington.xlsx', index=False)"
      ],
      "id": "98774662",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "cfb5b0e3"
      },
      "source": [
        "def cnn_br():\n",
        "    \n",
        "    lista_noticias=[]\n",
        "\n",
        "\n",
        "\n",
        "    url = (\"https://www.cnnbrasil.com.br/ultimas-noticias/\")\n",
        "\n",
        "    #acessando a url\n",
        "    driver.get(url)\n",
        "\n",
        "    #Esperando o carregamento\n",
        "    time.sleep(5)\n",
        "\n",
        "    #Procurando o elemento da página\n",
        "    element = driver.find_element_by_xpath(\"/html/body/div[2]/main/div/div[1]\")\n",
        "\n",
        "    #Carregando o html\n",
        "    html = element.get_attribute(\"innerHTML\")\n",
        "\n",
        "    #Utilizando o BeautifulSoup para acessar os dados\n",
        "    site = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "\n",
        "    noticias = site.findAll('a', attrs={'class': 'home__list__tag'}) \n",
        "\n",
        "    for noticia in noticias:\n",
        "    # Título\n",
        "        titulo = noticia.text.strip()\n",
        "\n",
        "\n",
        "        lista_noticias.append([\"cnn_br_ultimas\" ,titulo, '',noticia['href']])\n",
        "\n",
        "    news1 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo', 'Link'])\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    lista_noticias=[]\n",
        "\n",
        "\n",
        "\n",
        "    url = (\"https://www.cnnbrasil.com.br/\")\n",
        "\n",
        "    #acessando a url\n",
        "    driver.get(url)\n",
        "\n",
        "    #Esperando o carregamento\n",
        "    time.sleep(5)\n",
        "\n",
        "    #Procurando o elemento da página\n",
        "    element = driver.find_element_by_xpath(\"/html/body/div[2]\")\n",
        "\n",
        "    #Carregando o html\n",
        "    html = element.get_attribute(\"innerHTML\")\n",
        "\n",
        "    #Utilizando o BeautifulSoup para acessar os dados\n",
        "    site = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "\n",
        "    noticias = site.findAll('a', attrs={'class': 'home__post'}) \n",
        "\n",
        "    for noticia in noticias:\n",
        "    # Título\n",
        "        titulo = noticia.text.strip()\n",
        "\n",
        "\n",
        "        lista_noticias.append([\"cnn_br_home\" ,titulo, '',noticia['href']])\n",
        "\n",
        "    news2 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo', 'Link'])\n",
        "\n",
        "    df = pd.concat([news1 , news2])\n",
        "    df.to_excel('cnn_br.xlsx', index=False)"
      ],
      "id": "cfb5b0e3",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d399a56"
      },
      "source": [
        "def cnn_en():\n",
        "    lista_noticias=[]\n",
        "\n",
        "\n",
        "\n",
        "    url = (\"https://edition.cnn.com/\")\n",
        "\n",
        "    #acessando a url\n",
        "    driver.get(url)\n",
        "\n",
        "    #Esperando o carregamento\n",
        "    time.sleep(5)\n",
        "\n",
        "    #Procurando o elemento da página\n",
        "    element = driver.find_element_by_xpath(\"/html/body\")\n",
        "\n",
        "    #Carregando o html\n",
        "    html = element.get_attribute(\"innerHTML\")\n",
        "\n",
        "    #Utilizando o BeautifulSoup para acessar os dados\n",
        "    site = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "\n",
        "    noticias = site.findAll('div', attrs={'class': 'cd__content'}) \n",
        "\n",
        "    for noticia in noticias:\n",
        "    # Título\n",
        "        titulo = noticia.text\n",
        "        link = \"https://edition.cnn.com\"+noticia.find('a')['href']\n",
        "\n",
        "        lista_noticias.append([\"cnn_en\" ,titulo, '',link])\n",
        "\n",
        "    news1 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo', 'Link'])\n",
        "    news1 = news1[news1['Título'].str.len() > 30]\n",
        "    news1.to_excel('cnn_en.xlsx', index=False)"
      ],
      "id": "1d399a56",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "058f4a8d"
      },
      "source": [
        "def bbc_br():\n",
        "    \n",
        "    lista_noticias = []\n",
        "    url = \"http://www.bbc.co.uk/portuguese/index.xml\"\n",
        "    response = requests.get(url)\n",
        "    site = BeautifulSoup(response.content, features='xml')\n",
        "    noticias = site.findAll('item')\n",
        "\n",
        "    for noticia in noticias:\n",
        "      # Título\n",
        "        titulo = noticia.find('title')\n",
        "\n",
        "        # Subtítulo: div class=\"feed-post-body-resumo\"\n",
        "        subtitulo = re.sub(r'<.+?>', '', noticia.find('description').text).strip()\n",
        "\n",
        "        if (subtitulo):\n",
        "\n",
        "            lista_noticias.append([\"bbc_br\", titulo.text, subtitulo, noticia.find(\"link\").text])\n",
        "        else:\n",
        "            lista_noticias.append([\"bbc_br\" ,titulo.text, '', noticia.find(\"link\").text])\n",
        "\n",
        "    news2 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo', 'Link'])\n",
        "\n",
        "\n",
        "    news2.to_excel('bbc_br.xlsx', index=False)"
      ],
      "id": "058f4a8d",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cc48d41"
      },
      "source": [
        "def bbc_en():\n",
        "    \n",
        "    lista_noticias = []\n",
        "\n",
        "    url = (\"https://www.bbc.com/news\")\n",
        "\n",
        "    #acessando a url\n",
        "    driver.get(url)\n",
        "\n",
        "    #Esperando o carregamento\n",
        "    time.sleep(5)\n",
        "\n",
        "    #Procurando o elemento da página\n",
        "    element = driver.find_element_by_xpath(\"/html/body\")\n",
        "\n",
        "    #Carregando o html\n",
        "    html = element.get_attribute(\"innerHTML\")\n",
        "\n",
        "    #Utilizando o BeautifulSoup para acessar os dados\n",
        "    site = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "\n",
        "    principais = site.findAll('a', attrs={'class': 'gs-c-promo-heading gs-o-faux-block-link__overlay-link gel-paragon-bold nw-o-link-split__anchor'}) \n",
        "\n",
        "\n",
        "    for noticia in principais:\n",
        "        try:\n",
        "\n",
        "        # Título\n",
        "            titulo = noticia.text\n",
        "            link = \"https://www.bbc.com\"+noticia['href']\n",
        "\n",
        "            lista_noticias.append([\"bbc_en\" ,titulo, '',link])\n",
        "        except:\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "    noticias = site.findAll('a', attrs={'class':\"gs-c-promo-heading gs-o-faux-block-link__overlay-link gel-pica-bold nw-o-link-split__anchor\"})\n",
        "\n",
        "    for noticia in noticias:\n",
        "    # Título\n",
        "        titulo = noticia.text\n",
        "        link = \"https://www.bbc.com\"+noticia['href']\n",
        "\n",
        "        lista_noticias.append([\"bbc_en\" ,titulo, '',link])\n",
        "\n",
        "    news1 = pd.DataFrame(lista_noticias, columns=[\"fonte\",'Título', 'Subtítulo', 'Link']).drop_duplicates(subset= \"Título\",keep='last')\n",
        "    news1.to_excel('bbc_en.xlsx', index=False)"
      ],
      "id": "4cc48d41",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdc628ce"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    p1 = Process(target=g1())\n",
        "    p2 = Process(target=r7())\n",
        "    p3 = Process(target=folha())\n",
        "    p4 = Process(target=estadao())\n",
        "    p5 = Process(target=metro())\n",
        "    p6 = Process(target=globo())\n",
        "    p7 = Process(target=uol())\n",
        "    p8 = Process(target=terra())\n",
        "    p9 = Process(target=msn())\n",
        "    p10 = Process(target=buzz())\n",
        "    p11 = Process(target=veja())\n",
        "    p12 = Process(target=ig())\n",
        "    p13 = Process(target=antagonista())\n",
        "    p14 = Process(target=oglobo())\n",
        "    p15 = Process(target=ny_times())\n",
        "    p16 = Process(target=washington())\n",
        "    p17 = Process(target=cnn_br())\n",
        "    p18 = Process(target=cnn_en())\n",
        "    p19 = Process(target=bbc_br())\n",
        "    p20 = Process(target=bbc_en())\n",
        "    p1.start() \n",
        "    p2.start() \n",
        "    p3.start() \n",
        "    p4.start() \n",
        "    p5.start() \n",
        "    p6.start() \n",
        "    p7.start() \n",
        "    p8.start() \n",
        "    p9.start() \n",
        "    p10.start()\n",
        "    p11.start()\n",
        "    p12.start()\n",
        "    p13.start()\n",
        "    p14.start()\n",
        "    p15.start()\n",
        "    p16.start()\n",
        "    p17.start()\n",
        "    p18.start()\n",
        "    p19.start()\n",
        "    p20.start()\n"
      ],
      "id": "bdc628ce",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-WS6qVWihvO"
      },
      "source": [
        ""
      ],
      "id": "M-WS6qVWihvO",
      "execution_count": null,
      "outputs": []
    }
  ]
}